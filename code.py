# -*- coding: utf-8 -*-
"""Final_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PxmzdvJYG49S5sXNMf17ZZfcQbQf2VtT

Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install seaborn

import pandas as pd
import re
import numpy as np
import datetime as dt
from datetime import datetime, timedelta
from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, StratifiedKFold, cross_val_predict, train_test_split
from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, precision_recall_curve, auc, classification_report
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns

"""# =======================
# STEP 1: Load Raw Files
# =======================
"""

acctopop_raw = pd.read_excel("AcctoOpp.xlsx", header=10)
print(acctopop_raw.head())
pd.set_option('display.max_columns', acctopop_raw.shape[1])

oppo_raw = pd.read_excel("Oppo.xlsx", header=12)
pd.set_option('display.max_columns', oppo_raw.shape[1])
print(oppo_raw.head())

firmo_raw = pd.read_excel("Firmo.xlsx", header=9)
pd.set_option('display.max_columns', firmo_raw.shape[1])
print(firmo_raw.head())

intent_raw = pd.read_excel("Intent.xlsx", header=8)
pd.set_option('display.max_columns', intent_raw.shape[1])
print(intent_raw.head())

"""# =======================
# STEP 2: Drop Unnecessary Columns
# =======================
"""

columns_to_drop = ['Unnamed: 0', 'Unnamed: 2',]
acctopop_modified = acctopop_raw.drop(columns=columns_to_drop)

columns_to_drop1 = ['Unnamed: 0', 'Unnamed: 2', 'Owner','Date to Closed/Won', 'From Stage','Event Type',]
oppo_modified = oppo_raw.drop(columns=columns_to_drop1)

columns_to_drop4 = ['Unnamed: 0', 'Unnamed: 2',]
intent_modified = intent_raw.drop(columns=columns_to_drop4)

columns_to_drop6 = ['Unnamed: 0', 'Unnamed: 2','Employees',]
firmo_modified4 = firmo_raw.drop(columns=columns_to_drop6)

"""# =======================
# STEP 3: Remove NANS
# =======================

acctopop
"""

acctopop_cleaned = acctopop_modified.dropna()
# Define the columns you want to check for NaNs
columns_to_check = ['Opportunity Name', 'Opportunity ID']

# Remove rows where NaN values exist in *any* of the specified columns
acctopop= acctopop_cleaned.dropna(subset=columns_to_check)

# Verify that the NaNs are gone from these specific columns
print("NaN count after dropping rows with NaNs in Opportunity Name or Opportunity ID:")
print(acctopop[columns_to_check].isnull().sum())

print(acctopop.isnull().sum())

print(acctopop.head())

# Step 0: Make a full independent copy first
acctopop_cleaned1 = acctopop.copy()
contains_year_before_2020 = pd.Series([False] * len(acctopop_cleaned1), index=acctopop_cleaned1.index)
# Step 2: Loop through each column in the DataFrame
for col in acctopop_cleaned1.columns:
    # Check if the column's data type is 'object' (which typically means strings).
    # We only want to search for years within text columns.
    if acctopop_cleaned1[col].dtype == 'object':
        # Apply a function to each cell in the column to find and check years.
        # .fillna('') is crucial to convert any NaN values to empty strings before applying regex,
        # otherwise, re.findall might raise an error on NaN.
        # .astype(str) ensures all cell contents are treated as strings.
        column_has_old_year = acctopop_cleaned1[col].fillna('').astype(str).apply(
            lambda cell_content: any(
                # Use re.findall(r'\b\d{4}\b', cell_content) to find all 4-digit numbers.
                # \b ensures we match whole words (e.g., '2020' not '12020').
                # We then iterate through these found 'year_match' strings.
                # int(year_match) converts the string to an integer for comparison.
                # We check if this integer year is less than 2020.
                # The 'if year_match.isdigit()' guard ensures we only try to convert pure digits.
                int(year_match) < 2020
                for year_match in re.findall(r'\b\d{4}\b', cell_content)
                if year_match.isdigit()
            )
        )
        # Use |= (OR equals) to combine conditions across columns.
        # If any column for a row contains a year before 2020, that row is marked True.
        contains_year_before_2020 |= column_has_old_year

# Step 3: Apply the filter.
# We want to keep rows where 'contains_year_before_2020' is False
# (i.e., NO years before 2020 were found in any text column).
# The '~' operator means "NOT".
acctopop_cleaned1 = acctopop_cleaned1[~contains_year_before_2020]

print("--- Filtered acctopop DataFrame (Only years 2020 and above in text columns) ---")
print(acctopop_cleaned1)
print(f"\nFiltered DataFrame shape: {acctopop_cleaned1.shape}")

print(acctopop_cleaned1.isnull().sum())

# Number of non-null rows in the 'Opportunity ID' column
num_rows = acctopop_cleaned1['Opportunity ID'].notnull().sum()
print(f"Number of non-null rows in 'Opportunity ID': {num_rows}")

print(acctopop_cleaned1.columns.tolist())

"""opporaw"""

print(oppo_modified.isnull().sum())

oppo_cleaned = oppo_modified
# Define the columns you want to check for NaNs
columns_to_check1 = ['Opportunity ID', 'Opportunity Name', 'Account Name', 'To Stage',
    'Won', 'Opportunity Close Date', 'Vertical', 'Stage Change', 'Last Stage Change Date',]

# Remove rows where NaN values exist in *any* of the specified columns
oppo= oppo_cleaned.dropna(subset=columns_to_check1)

# Verify that the NaNs are gone from these specific columns
print("NaN count after dropping rows with NaNs:")
print(oppo[columns_to_check1].isnull().sum())

print(oppo.isnull().sum())

# Keep rows that are >= 2020

print("--- Before Filtering ---")
print(f"Original DataFrame shape: {oppo.shape}")
# Check the data type of the column before conversion (it should be 'object' or 'string')
print(f"Data type of 'Opportunity Close Date' BEFORE conversion: {oppo['Opportunity Close Date'].dtype}")
# Show the first few dates in the column
print("First 5 'Opportunity Close Date' values (before conversion):")
print(oppo['Opportunity Close Date'].head())
print("-" * 40)

oppo_cleaned1 = oppo.copy()
oppo_cleaned1['Opportunity Close Date'] = pd.to_datetime(oppo_cleaned1['Opportunity Close Date'], errors='coerce')
date_filter_condition = (oppo_cleaned1['Opportunity Close Date'].notna()) & \
                        (oppo_cleaned1['Opportunity Close Date'] >= '2020-01-01')
text_filter_condition = pd.Series([False] * len(oppo_cleaned1), index=oppo_cleaned1.index)
# Loop through each column in the DataFrame
for col in oppo_cleaned1.columns:
    # Check if the column's data type is 'object' (which usually means strings)
    if oppo_cleaned1[col].dtype == 'object':
        # Apply the string contains filter.
        # .astype(str) handles potential non-string objects (like numbers) by converting them to string.
        # .str.contains('2019', na=False) checks if the string contains '2019'.
        # na=False treats NaN values as not containing '2019'.
        # The '|' (OR) operator combines the conditions: if '2019' is found in *any* string column,
        # that row's text_filter_condition becomes True.
        text_filter_condition = text_filter_condition | \
                                oppo_cleaned1[col].astype(str).str.contains('2019', na=False)

# Step 4: Combine both conditions
# We want to keep rows that satisfy the date condition AND DO NOT satisfy the text_filter_condition.
# So, we want (date_filter_condition) AND (NOT text_filter_condition)
final_filter = date_filter_condition & (~text_filter_condition)

# Step 5: Apply the combined filter to the DataFrame
oppo_cleaned1 = oppo_cleaned1[final_filter]

print(oppo_cleaned1)

print(oppo_cleaned1.isnull().sum())

oppo_cleaned1 = oppo_cleaned1.dropna(subset=['Date to Target'])
print(oppo_cleaned1.isnull().sum())

# Define the list of original date duration columns
original_date_cols = [
    'Date to Detected',
    'Date to Engaged',
    'Date to Prioritized',
    'Date to Qualified',
    'Date to Sales Stages'
]

print("--- Before Processing All Columns ---")
print("Original DataFrame shape:", oppo_cleaned1.shape)
print("NaN counts per original column:\n", oppo_cleaned1[original_date_cols].isna().sum())


# Create the binary indicator columns and impute 0 for NaNs in original columns
for col in original_date_cols:
    # Construct the new binary column name (e.g., 'Binary Date to Detected')
    binary_col_name = 'Binary ' + col

    # Create the binary indicator column (1 if reached, 0 if NaN/not reached)
    oppo_cleaned1[binary_col_name] = oppo_cleaned1[col].notna().astype(int)

    # Impute NaN values in the original column with 0
    oppo_cleaned1[col] = oppo_cleaned1[col].fillna(0)


print("\n--- After Processing All Columns ---")
print("New DataFrame shape (should be the same):", oppo_cleaned1.shape)
print("NaN counts per original column after imputation (should all be 0):\n", oppo_cleaned1[original_date_cols].isna().sum())

# Display the first few rows of the DataFrame to show changes and new columns
print("\nFirst 5 rows of oppo_cleaned1 after processing (showing original and new binary columns):")

oppo_cleaned1.columns

print(oppo_cleaned1.isnull().sum())

print("--- Unique values in 'To Stage' column of oppo_cleaned1 ---")
# Use .value_counts() to see all unique stages and their frequency
# dropna=False includes any NaN values if present, though you confirmed no NaNs earlier.
print(oppo_cleaned1['To Stage'].value_counts(dropna=False))

print("\n--- Check complete ---")

print("--- Starting Aggregation of oppo_cleaned1 (with precise Final_To_Stage) ---")
print(f"Original oppo_cleaned1 shape: {oppo_cleaned1.shape}")
print(f"Unique Opportunity IDs in original oppo_cleaned1: {oppo_cleaned1['Opportunity ID'].nunique()}")


# Define the logical order of your stages with numerical ranks
# Higher number means a later/more advanced stage
stage_order_mapping = {
    'Research': 1,
    'Target Demand': 2,
    'Detected Demand': 3,
    'Active Demand': 4,
    'Engaged Demand': 5,
    'Prioritized Demand': 6,
    'Qualified Demand': 7,
    'Discovery+': 8,
    'Closed Lost': 9,    # A terminal stage, ranked high
    'Closed Won': 10     # The ultimate successful terminal stage, highest rank
}

# Define the custom function to find the highest stage reached within a group
def get_highest_stage_reached(stage_series):
    # Map the stage names in the current opportunity's history to their numerical ranks
    ranks = stage_series.map(stage_order_mapping)

    # Find the maximum rank achieved by this opportunity
    # .max() will correctly ignore any NaNs if they somehow appear (though we cleaned them)
    highest_rank = ranks.max()

    # If all stages for an opportunity were not in our defined mapping (unlikely but safe check)
    if pd.isna(highest_rank):
        return np.nan # Or 'Unknown Stage' if you prefer a string

    # Find the stage name that corresponds to the highest rank
    # We iterate through the mapping to find the key for the max rank value
    highest_stage_name = None
    for stage_name, rank_value in stage_order_mapping.items():
        if rank_value == highest_rank:
            highest_stage_name = stage_name
            break # Found the stage name for the highest rank

    return highest_stage_name


# Perform the aggregation, storing the result in aggregated_oppo1
aggregated_oppo1 = oppo_cleaned1.groupby('Opportunity ID').agg(
    # Basic Opportunity Info
    Opportunity_Name=('Opportunity Name', 'first'),
    Account_Name=('Account Name', 'first'),

    # Outcome & Dates
    Won=('Won', 'max'),
    Opportunity_Close_Date=('Opportunity Close Date', 'max'),
    Last_Stage_Change_Date=('Last Stage Change Date', 'max'),

    # NEW: Using the custom function for Final_To_Stage
    Final_To_Stage=('To Stage', get_highest_stage_reached),

    # Count of Activities
    Total_Stage_Changes=('Opportunity ID', 'count'),

    # Duration features (assuming NaNs were handled, e.g., filled with 0)
    Max_Date_to_Target=('Date to Target', 'max'),
    Max_Date_to_Detected=('Date to Detected', 'max'),
    Max_Date_to_Engaged=('Date to Engaged', 'max'),
    Max_Date_to_Prioritized=('Date to Prioritized', 'max'),
    Max_Date_to_Qualified=('Date to Qualified', 'max'),
    Max_Date_to_Sales_Stages=('Date to Sales Stages', 'max'),

    # Binary flags (whether stage was ever reached)
    Ever_Reached_Detected=('Binary Date to Detected', 'max'),
    Ever_Reached_Engaged=('Binary Date to Engaged', 'max'),
    Ever_Reached_Prioritized=('Binary Date to Prioritized', 'max'),
    Ever_Reached_Qualified=('Binary Date to Qualified', 'max'),
    Ever_Reached_Sales_Stages=('Binary Date to Sales Stages', 'max')

).reset_index()


print("\n--- Aggregation Complete for oppo_cleaned1 (Revised Final_To_Stage)! ---")
print(f"New aggregated_oppo1 shape: {aggregated_oppo1.shape}")
print(f"Unique Opportunity IDs in aggregated_oppo1: {aggregated_oppo1['Opportunity ID'].nunique()}")

print("\nFirst 5 rows of aggregated_oppo1 (showing key aggregated features including Final_To_Stage):")
pd.set_option('display.max_columns', None)
print(aggregated_oppo1.head())
pd.reset_option('display.max_columns')

print("\n'aggregated_oppo1' now contains one row per unique opportunity with the highest stage reached based on your defined order.")

print("--- Displaying all columns of aggregated_oppo1 (first 5 rows) ---")

# Set pandas option to display all columns
pd.set_option('display.max_columns', None)

# Display the head of the aggregated_oppo1 DataFrame
print(aggregated_oppo1.head())

aggregated_oppo1.columns

# Rename the columns
aggregated_oppo1 = aggregated_oppo1.rename(columns={
    'Account_Name': 'Account Name',
    'Opportunity_Name': 'Opportunity Name'
})

print("\n--- Renaming complete ---")
print("New columns of aggregated_oppo1 (first 5):\n", aggregated_oppo1.columns.tolist()[:5])

print("\nFirst 5 rows of aggregated_oppo1 with new column names:")
pd.set_option('display.max_columns', None)
print(aggregated_oppo1.head())
pd.reset_option('display.max_columns')

# Number of non-null rows in the 'Opportunity ID' column
num_rows1 = aggregated_oppo1['Opportunity ID'].notnull().sum()
print(f"Number of non-null rows in 'Opportunity ID': {num_rows1}")

"""Intent clean and aggregiate"""

intent_modified

intent_modified = intent_modified.dropna()

print(intent_modified.isnull().sum())

# Rename the columns
intent_clean = intent_modified.rename(columns={
    'Account: Account ID': 'Account ID',
    'Account: Account Name': 'Account Name'
})

intent_clean.columns

intent_clean

print(intent_clean.columns)

print("--- Starting Aggregation of intent_clean (using .apply() for robustness) ---")
print(f"Original intent_clean shape: {intent_clean.shape}")
print(f"Unique Accounts in original intent_clean: {intent_clean['Account ID'].nunique()}")


# Define a single custom function that will perform ALL aggregations for one group
def aggregate_account_data(group_df):
    # 'group_df' here is a mini-DataFrame containing all rows for a single Account ID/Account Name combination

    # 1. Aggregate Signal Score:
    mean_score = group_df['Signal Score'].mean()
    max_score = group_df['Signal Score'].max()

    # 2. Aggregate Topics:
    all_unique_topics = list(group_df['Topic'].unique()) # Get a list of all distinct topics
    num_unique_topics = group_df['Topic'].nunique()       # Count them

    # 3. Find Topic(s) with Max Score:
    max_score_value = group_df['Signal Score'].max()
    # Find all topics where the 'Signal Score' is equal to the max_score_value
    topics_at_max_score = group_df[group_df['Signal Score'] == max_score_value]['Topic'].unique()
    topic_with_max_score = list(topics_at_max_score) # Return as a list

    # Return a pandas Series where each element will become a column in the final DataFrame
    return pd.Series({
        'Mean_Signal_Score': mean_score,
        'Max_Signal_Score': max_score,
        'All_Unique_Topics': all_unique_topics,
        'Num_Unique_Topics': num_unique_topics,
        'Topic_With_Max_Score': topic_with_max_score
    })


# Perform the aggregation using .apply()
# group_keys=False prevents the grouped columns from becoming a new index level
aggregated_intent = intent_clean.groupby(['Account ID', 'Account Name'], group_keys=False).apply(aggregate_account_data)

# Resetting the index to turn 'Account ID' and 'Account Name' back into regular columns
# This is needed because .apply() on groupby often preserves the group keys as index.
aggregated_intent = aggregated_intent.reset_index()


print("\n--- Aggregation Complete! ---")
print(f"New aggregated_intent shape: {aggregated_intent.shape}")
print(f"Unique Accounts in aggregated_intent: {aggregated_intent['Account ID'].nunique()}")

print("\nFirst 5 rows of aggregated_intent (showing all columns):")
pd.set_option('display.max_columns', None) # Display all columns
print(aggregated_intent.head())
pd.reset_option('display.max_columns') # Reset display option

print("\nAggregation successful! 'aggregated_intent' now contains one row per unique account.")

"""firmo (remove nans, aggregiate, remove non icp)"""

firmo_modified4.columns

# Count rows where 'Type' column contains 'Customer'
customer_count2 = firmo_modified4[firmo_modified4['Type'] == 'Customer'].shape[0]
print(f"Number of rows with 'Customer' in 'Type' column: {customer_count2}")

# Select rows where the 'Type' column is exactly 'Customer'
customer_rows = firmo_modified4[firmo_modified4['Type'] == 'Customer']

# Display the first few (or all, if 101 is manageable) of these rows
# If 101 rows is a lot, you might just want to see the head or sample
print(customer_rows.head())
# To see how many rows were actually selected (should match your 101)
print(f"\nNumber of rows found with 'Customer' in 'Type' column: {len(customer_rows)}")

# First, get the 'Customer' rows
customer_rows = firmo_modified4[firmo_modified4['Type'] == 'Customer'].copy() # Use .copy() to avoid SettingWithCopyWarning if you modify it later

print(f"Number of 'Customer' rows found: {len(customer_rows)}")

# Check for duplicates across all columns in these 'Customer' rows
# .duplicated() returns a boolean Series, True for duplicate rows (after the first occurrence)
duplicate_customer_rows_count = customer_rows.duplicated().sum()

print(f"Number of duplicate 'Customer' rows (based on all columns): {duplicate_customer_rows_count}")

if duplicate_customer_rows_count == 0:
    print("\nAll 'Customer' rows are unique across all columns.")
else:
    print(f"\nThere are {duplicate_customer_rows_count} duplicate 'Customer' rows.")
    print("Here are the first 5 duplicate rows (excluding their first occurrence):")
    print(customer_rows[customer_rows.duplicated()].head())

print(firmo_modified4.isnull().sum())

# Define the columns you want to check for NaNs
columns_to_check8 = ['Account ID', 'Account Name', 'Type', 'Annual Revenue','Billing Country', 'Industry','Mktg Priority','Total IAO Value']

# Remove rows where NaN values exist in *any* of the specified columns
firmo_clean1= firmo_modified4.dropna(subset=columns_to_check8)

# Verify that the NaNs are gone from these specific columns
print(firmo_clean1.isnull().sum())

# Count rows where 'Type' column contains 'Customer'
customer_count1 = firmo_clean1[firmo_clean1['Type'] == 'Customer'].shape[0]
print(f"Number of rows with 'Customer' in 'Type' column: {customer_count1}")

# Define the list of columns where you want to fill NaNs with 0
columns_to_fill_with_zero = [
    'Engaged People',
    'Engagement Minutes (3 mo.)',
    'Adobe Technologies Used',
    'CVENT Technologies Used',
    'Ability to Pay'
]
# Fill NaN values with 0 in the specified columns
for col in columns_to_fill_with_zero:
    firmo_clean1[col] = firmo_clean1[col].fillna(0)

print(firmo_clean1.isnull().sum())

# Count rows where 'Type' column contains 'Customer'
customer_count1 = firmo_clean1[firmo_clean1['Type'] == 'Customer'].shape[0]
print(f"Number of rows with 'Customer' in 'Type' column: {customer_count1}")



total_rows = firmo_clean1.shape[0]
print(f"Total number of rows: {total_rows}")
non_icp_count = firmo_clean1[firmo_clean1['Mktg Priority'] == 'Non-ICP'].shape[0]
print(f"Number of rows with 'Non-ICP' in 'Mktg Priority': {non_icp_count}")

firmo_clean1 = firmo_clean1[firmo_clean1['Mktg Priority'] != 'Non-ICP']


print("\n--- After Removing 'Non-ICP' Rows ---")
print("New DataFrame shape:", firmo_clean1.shape)

# Count rows where 'Type' column contains 'Customer'
customer_count3 = firmo_clean1[firmo_clean1['Type'] == 'Customer'].shape[0]
print(f"Number of rows with 'Customer' in 'Type' column: {customer_count1}")

# Define the columns to transform
tech_columns = [
    'Adobe Technologies Used',
    'CVENT Technologies Used'
]
# Transform each technology column into a count
for col in tech_columns: # This loop ensures only columns in 'tech_columns' are touched
    # Convert to string and handle explicit 'nan' string (harmless if no NaNs)
    firmo_clean1[col] = firmo_clean1[col].astype(str).replace('nan', '')

    # Handle cases where "None" or "none" might be explicitly written
    firmo_clean1[col] = firmo_clean1[col].replace(['None', 'none'], '')

    # Apply a function to count the technologies, splitting by '; '
    firmo_clean1[col] = firmo_clean1[col].apply(
        lambda x: len([item for item in x.split('; ') if item.strip()])
    )

pd.set_option('display.max_columns', None)
print(firmo_clean1.sample(5)) # Displays 5 random rows

# Count rows where 'Type' column contains 'Customer'
customer_count5 = firmo_clean1[firmo_clean1['Type'] == 'Customer'].shape[0]
print(f"Number of rows with 'Customer' in 'Type' column: {customer_count5}")

## clean vertical

# Fill NaN values in 'Vertical' column with 'Unknown'
firmo_clean1['Vertical'] = firmo_clean1['Vertical'].fillna('Unknown')


print("\n--- After handling NaNs in 'Vertical' ---")
print("NaN count in 'Vertical' after fill (should be 0):", firmo_clean1['Vertical'].isna().sum())
print("Value counts for 'Vertical' after fill:\n", firmo_clean1['Vertical'].value_counts(dropna=False).head()) # 'Unknown' should now appear

firmo_clean1

##Merging

print("--- DataFrame Shapes ---")

# Check shape for intent_clean
if 'intent_clean' in locals() and isinstance(aggregated_intent, pd.DataFrame):
    print(f"intent_clean: {aggregated_intent.shape[0]} rows, {aggregated_intent.shape[1]} columns")
else:
    print("Warning: intent_clean DataFrame not found or not a DataFrame.")

# Check shape for firmo_clean
if 'firmo_clean' in locals() and isinstance(firmo_clean, pd.DataFrame):
    print(f"firmo_clean: {firmo_clean.shape[0]} rows, {firmo_clean.shape[1]} columns")
else:
    print("Warning: firmo_clean DataFrame not found or not a DataFrame.")

# Check shape for oppo_cleaned1
if 'oppo_cleaned1' in locals() and isinstance(oppo_cleaned1, pd.DataFrame):
    print(f"oppo_cleaned1: {oppo_cleaned1.shape[0]} rows, {oppo_cleaned1.shape[1]} columns")
else:
    print("Warning: oppo_cleaned1 DataFrame not found or not a DataFrame.")

# Check shape for acctopop_cleaned1
if 'acctopop_cleaned1' in locals() and isinstance(acctopop_cleaned1, pd.DataFrame):
    print(f"acctopop_cleaned1: {acctopop_cleaned1.shape[0]} rows, {acctopop_cleaned1.shape[1]} columns")
else:
    print("Warning: acctopop_cleaned1 DataFrame not found or not a DataFrame.")

# Check nulls for firmo_clean
if 'firmo_clean' in locals() and isinstance(firmo_clean1, pd.DataFrame):
    print("\n--- Null values in firmo_clean ---")
    print(firmo_clean1.isnull().sum())
else:
    print("\nWarning: firmo_clean DataFrame not found or not a DataFrame. Cannot check nulls.")

# --- Define the potential identifier columns ---
potential_id_cols = ['Account ID', 'Opportunity ID']

# --- Create the dictionary of DataFrames to check ---
dataframes_to_check = {
    "aggregated_intent": aggregated_intent,
    "firmo_clean1": firmo_clean1,
    "aggregated_oppo1": aggregated_oppo1, # <-- Checking the aggregated version of oppo data
    "acctopop_cleaned1": acctopop_cleaned1
}

print("--- Final Check: Unique Identifiers Before Merge ---")

for df_name, df in dataframes_to_check.items():
    print(f"\n--- Checking {df_name} ---")
    if not isinstance(df, pd.DataFrame):
        print(f"Warning: {df_name} is not a loaded DataFrame (or not a pandas DataFrame object). Skipping.")
        continue

    df_rows = df.shape[0]
    print(f"Total rows in {df_name}: {df_rows}")

    found_unique_id = False
    for id_col in potential_id_cols:
        if id_col in df.columns:
            unique_ids = df[id_col].nunique()
            print(f"  Column '{id_col}' found. Unique values: {unique_ids}")

            if unique_ids == df_rows:
                print(f"  --> '{id_col}' is a UNIQUE IDENTIFIER for {df_name} (Number of unique IDs matches total rows).")
                found_unique_id = True
            else:
                print(f"  --> '{id_col}' is NOT a unique identifier for {df_name} (Has duplicates: {df_rows - unique_ids} duplicates).")
        else:
            print(f"  Column '{id_col}' not found in {df_name}.")

    if not found_unique_id:
        print(f"  Consider checking other columns or a combination of columns for a unique identifier in {df_name}.")
        print(f"  Available columns in {df_name}: {df.columns.tolist()}")

import pandas as pd

# Assuming aggregated_oppo1 and acctopop_cleaned1 are loaded in your environment.
# (They should be from your previous steps)

print("--- Starting Merge: acctopop_cleaned1 (base opps) + aggregated_oppo1 (opp history) ---")
print(f"acctopop_cleaned1 shape before merge: {acctopop_cleaned1.shape}")
print(f"aggregated_oppo1 shape before merge: {aggregated_oppo1.shape}")


# Perform the left merge on 'Opportunity ID'
merged_opportunities_data1 = pd.merge(
    acctopop_cleaned1,     # Left DataFrame
    aggregated_oppo1,      # Right DataFrame
    on='Opportunity ID',   # Merge key
    how='left'
)


print("\n--- Merge 1 Complete! ---")
print(f"Merged DataFrame (merged_opportunities_data1) shape: {merged_opportunities_data1.shape}")

# --- CORRECTED: Check for NaNs using _y suffixes for potentially conflicting columns ---
print("\nNaNs introduced for aggregated_oppo1 features after merge (expected if some opportunities had no detailed history):")

# Get columns that came from aggregated_oppo1 (the right DataFrame)
# These will either have _y suffix if name conflict, or no suffix if unique to aggregated_oppo1
# We iterate through the original columns of aggregated_oppo1 and add _y if they are not the merge key
columns_from_agg_oppo1_after_merge = []
for col in aggregated_oppo1.columns:
    if col == 'Opportunity ID':
        continue # Merge key doesn't get suffix
    elif col in acctopop_cleaned1.columns: # If the column also existed in the left DF
        columns_from_agg_oppo1_after_merge.append(f"{col}_y")
    else: # If the column was unique to aggregated_oppo1
        columns_from_agg_oppo1_after_merge.append(col)

# Now check for NaNs only in these columns in the merged DataFrame
print(merged_opportunities_data1[columns_from_agg_oppo1_after_merge].isnull().sum())

# --- End of Correction ---


print("\nFirst 5 rows of merged_opportunities_data1:")
# Display all columns to see the merged data
pd.set_option('display.max_columns', None)
print(merged_opportunities_data1.head())
pd.reset_option('display.max_columns')

print("\n'merged_opportunities_data1' now combines unique opportunity records with their aggregated history.")
print("This is Step 2 of your overall plan.")

merged_opportunities_data1

# Assuming merged_opportunities_data1 is loaded in your environment.
# (It was just created in the previous step)

print("--- Starting Aggregation of merged_opportunities_data1 by Account ID ---")
print(f"Original merged_opportunities_data1 shape: {merged_opportunities_data1.shape}")
print(f"Unique Account IDs in original merged_opportunities_data1: {merged_opportunities_data1['Account ID'].nunique()}")


# --- IMPORTANT: Convert date columns to datetime objects BEFORE aggregation ---
# This is critical for max/min to work correctly on dates, and for handling NaNs.
# Note: These are columns from aggregated_oppo1 that came into merged_opportunities_data1
date_cols_to_convert_in_merge = [
    'Opportunity_Close_Date', 'Last_Stage_Change_Date', # Already converted these
    'Max_Date_to_Target', 'Max_Date_to_Detected', 'Max_Date_to_Engaged',
    'Max_Date_to_Prioritized', 'Max_Date_to_Qualified', 'Max_Date_to_Sales_Stages'
]

for col in date_cols_to_convert_in_merge:
    if col in merged_opportunities_data1.columns:
        merged_opportunities_data1[col] = pd.to_datetime(merged_opportunities_data1[col], errors='coerce')
        # print(f"Converted '{col}' to datetime type.")
    else:
        print(f"Warning: Column '{col}' not found in merged_opportunities_data1. Skipping conversion for it.")


# Define the logical order of your stages with numerical ranks (same as before)
stage_order_mapping = {
    'Research': 1, 'Target Demand': 2, 'Detected Demand': 3, 'Active Demand': 4,
    'Engaged Demand': 5, 'Prioritized Demand': 6, 'Qualified Demand': 7,
    'Discovery+': 8, 'Closed Lost': 9, 'Closed Won': 10
}

# Define the custom function to find the highest stage reached for an account (across its opportunities)
def get_highest_stage_for_account(final_to_stage_series):
    stages_to_consider = final_to_stage_series.dropna()
    if stages_to_consider.empty:
        return np.nan

    ranks = stages_to_consider.map(stage_order_mapping)
    ranks = ranks.dropna()

    if ranks.empty:
        return np.nan

    highest_rank = ranks.max()
    highest_stage_name = None
    for stage_name, rank_value in stage_order_mapping.items():
        if rank_value == highest_rank:
            highest_stage_name = stage_name
            break

    return highest_stage_name


# Perform the aggregation
aggregated_opportunities_by_account = merged_opportunities_data1.groupby(['Account ID']).agg(
    # Take the first Account Name (from acctopop_cleaned1 side)
    Account_Name=('Account Name_x', 'first'),

    # Summarize opportunities per account
    Total_Opportunities=('Opportunity ID', 'count'),
    Total_Won_Opportunities=('Won', lambda x: x.sum() if x.notna().any() else 0),
    Total_Lost_Opportunities=('Won', lambda x: x[x==0].count() if x.notna().any() else 0),

    # Summarize stage progression across all opportunities for the account
    Highest_Stage_Reached_Overall=('Final_To_Stage', get_highest_stage_for_account),

    # Summarize activity dates (across all opportunities for the account) - NOW THEY ARE DATETIME OBJECTS
    Latest_Opportunity_Close_Date=('Opportunity_Close_Date', 'max'),
    Latest_Activity_Date=('Last_Stage_Change_Date', 'max'),

    # --- CORRECTED: Changed 'mean' to 'max' for date columns ---
    Max_Stage_Target_Date=('Max_Date_to_Target', 'max'), # Renamed for clarity: now it's a date
    Max_Stage_Detected_Date=('Max_Date_to_Detected', 'max'), # Renamed for clarity
    Max_Stage_Engaged_Date=('Max_Date_to_Engaged', 'max'), # Renamed for clarity
    Max_Stage_Prioritized_Date=('Max_Date_to_Prioritized', 'max'), # Renamed for clarity
    Max_Stage_Qualified_Date=('Max_Date_to_Qualified', 'max'), # Renamed for clarity
    Max_Stage_Sales_Stages_Date=('Max_Date_to_Sales_Stages', 'max'), # Renamed for clarity

    # Summarize 'ever reached' flags (max across all opportunities for the account)
    Ever_Reached_Detected_Overall=('Ever_Reached_Detected', 'max'),
    Ever_Reached_Engaged_Overall=('Ever_Reached_Engaged', 'max'),
    Ever_Reached_Prioritized_Overall=('Ever_Reached_Prioritized', 'max'),
    Ever_Reached_Qualified_Overall=('Ever_Reached_Qualified', 'max'),
    Ever_Reached_Sales_Stages_Overall=('Ever_Reached_Sales_Stages', 'max')

).reset_index()


print("\n--- Aggregation Complete for merged_opportunities_data1 by Account ID! ---")
print(f"New aggregated_opportunities_by_account shape: {aggregated_opportunities_by_account.shape}")
print(f"Unique Account IDs in aggregated_opportunities_by_account: {aggregated_opportunities_by_account['Account ID'].nunique()}")

print("\nFirst 5 rows of aggregated_opportunities_by_account (showing key aggregated features):")
pd.set_option('display.max_columns', None)
print(aggregated_opportunities_by_account.head())
pd.reset_option('display.max_columns')

print("\n'aggregated_opportunities_by_account' now contains one row per unique account, summarizing all its opportunities.")
print("This is Step 3 of your overall plan.")

aggregated_opportunities_by_account

# Assuming aggregated_opportunities_by_account is already loaded/created in your environment.

print("--- Starting Cleanup of 1970-01-01 dates in aggregated_opportunities_by_account ---")

# List of the date columns in aggregated_opportunities_by_account that might contain 1970-01-01 artifacts
date_cols_to_clean = [
    'Max_Stage_Target_Date',
    'Max_Stage_Detected_Date',
    'Max_Stage_Engaged_Date',
    'Max_Stage_Prioritized_Date',
    'Max_Stage_Qualified_Date',
    'Max_Stage_Sales_Stages_Date',
    'Latest_Opportunity_Close_Date',
    'Latest_Activity_Date'
]

# The 1970-01-01 date is often stored as a datetime object, possibly with UTC timezone.
# We need to compare it correctly.
# A robust way is to check year, month, day.

for col in date_cols_to_clean:
    if col in aggregated_opportunities_by_account.columns:
        # Ensure the column is datetime type first, coercing errors to NaT
        aggregated_opportunities_by_account[col] = pd.to_datetime(aggregated_opportunities_by_account[col], errors='coerce')

        # Use boolean indexing to find and replace 1970-01-01 dates with NaT
        # This checks if the date is not NaT AND its year/month/day match 1970-01-01
        aggregated_opportunities_by_account.loc[
            aggregated_opportunities_by_account[col].notna() & # Only process non-NaT values
            (aggregated_opportunities_by_account[col].dt.year == 1970) &
            (aggregated_opportunities_by_account[col].dt.month == 1) &
            (aggregated_opportunities_by_account[col].dt.day == 1), col
        ] = pd.NaT # Replace with NaT (Not a Time)

        print(f"Cleaned 1970-01-01 values in '{col}'.")
    else:
        print(f"Warning: Column '{col}' not found in aggregated_opportunities_by_account. Skipping cleanup for it.")

print("\n--- 1970-01-01 date cleanup complete in aggregated_opportunities_by_account ---")

# Optional: Display head to confirm the change
# pd.set_option('display.max_columns', None)
# print(aggregated_opportunities_by_account[date_cols_to_clean].head())
# pd.reset_option('display.max_columns')

# Step 4a: Merge firmo_clean1 with aggregated_intent
# Result: Merged firmographics + intent data
merged_account_base_data = pd.merge(
    firmo_clean1,
    aggregated_intent,
    on='Account ID',
    how='left'
)
print(f"\nShape after merging firmo_clean1 + aggregated_intent: {merged_account_base_data.shape}")

# Step 4b: Merge the result with aggregated_opportunities_by_account
# Result: Final Master DataFrame with all data at the Account ID level
final_merged_data = pd.merge(
    merged_account_base_data, # Left DF (firmographics + intent)
    aggregated_opportunities_by_account, # Right DF (summarized opps by account)
    on='Account ID',
    how='left'
)
print(f"\nShape after merging with aggregated_opportunities_by_account: {final_merged_data.shape}")

print("\n--- Final Merged DataFrames Complete! ---")

# Check for NaNs introduced by the merges for the new features
print("\nNaNs introduced for newly merged features (expected for unmatched accounts):")
# Columns from aggregated_intent:
intent_cols = ['Mean_Signal_Score', 'Max_Signal_Score', 'All_Unique_Topics', 'Num_Unique_Topics', 'Topic_With_Max_Score']
# Columns from aggregated_opportunities_by_account:
opp_by_acc_cols = [col for col in aggregated_opportunities_by_account.columns if col not in ['Account ID', 'Account Name']]
# Account_Name will have _x/_y from earlier merges, need to be careful.
# Let's just check a few representative ones.
check_cols = ['Mean_Signal_Score', 'Max_Signal_Score', 'Total_Opportunities', 'Highest_Stage_Reached_Overall']

print(final_merged_data[check_cols].isnull().sum())


print("\nFirst 5 rows of final_merged_data:")
pd.set_option('display.max_columns', None)
print(final_merged_data.head())
pd.reset_option('display.max_columns')

print("\nYour 'final_merged_data' DataFrame now contains all information at the unique account level.")
print("This is the main dataset you can use for your scoring model!")

final_merged_data

final_merged_data.columns

import pandas as pd

# Assuming final_merged_data is already loaded/created in your environment.

print("--- Columns in final_merged_data RIGHT BEFORE Polishing Step ---")
print(final_merged_data.columns.tolist())

# Load your final_merged_data (if not already loaded)
# final_merged_data = pd.read_csv('path/to/your_final_merged_data.csv')

# These lines usually run at the very start of the polishing block
print("--- Starting Polishing of final_merged_data ---")
print(f"Original final_merged_data shape: {final_merged_data.shape}")
print(f"Columns before polishing:\n{final_merged_data.columns.tolist()}")

# --- Re-run Numerical cleaning first (if you restart the whole polishing section) ---
# This ensures numbers are properly handled before the list/date steps.
numerical_cols_to_fill_0 = [
    'Mean_Signal_Score', 'Max_Signal_Score', 'Num_Unique_Topics',
    'Total_Opportunities', 'Total_Won_Opportunities', 'Total_Lost_Opportunities',
    'Engaged People', 'Engagement Minutes (3 mo.)',
    'Adobe Technologies Used', 'CVENT Technologies Used',
    'Total IAO Value', 'Ability to Pay',
    'Ever_Reached_Detected_Overall', 'Ever_Reached_Engaged_Overall',
    'Ever_Reached_Prioritized_Overall', 'Ever_Reached_Qualified_Overall',
    'Ever_Reached_Sales_Stages_Overall'
]
for col in numerical_cols_to_fill_0:
    if col in final_merged_data.columns:
        final_merged_data[col] = pd.to_numeric(final_merged_data[col], errors='coerce')
        final_merged_data[col] = final_merged_data[col].fillna(0)
    else:
        print(f"Warning: Numerical column '{col}' not found for filling with 0.")

# Make sure Account Name is correctly consolidated as well
# (This block was removed from the final code, but if you ran previous versions,
# ensure 'Account Name' is correctly set and old suffixed columns are dropped if present)
if 'Account Name_x' in final_merged_data.columns:
    final_merged_data['Account Name'] = final_merged_data['Account Name_x']
    final_merged_data = final_merged_data.drop(columns=['Account Name_x', 'Account Name_y', 'Account_Name'], errors='ignore')

# --- 0. Initial Setup & Load Confirmation ---
# Assuming final_merged_data is already loaded/created in your environment.
# (If you restarted your session, you'll need to re-run ALL previous data loading
# and merging/aggregation steps to create final_merged_data first)

print("--- Starting Polishing Process (Step by Step) ---")
print(f"Original final_merged_data shape: {final_merged_data.shape}")
print(f"Columns before polishing:\n{final_merged_data.columns.tolist()}")
print("\n--- Initial State Checked ---")

# --- Step 1: Consolidate 'Account Name' Columns ---
# Keep 'Account Name' as the primary 'Account Name' from the _x side (firmo_clean1)
# This block is needed because Account Name_x, Account Name_y, Account_Name exist.
print("\n--- Running Step 1: Consolidate 'Account Name' Columns ---")

if 'Account Name_x' in final_merged_data.columns:
    final_merged_data['Account Name'] = final_merged_data['Account Name_x']
    final_merged_data = final_merged_data.drop(columns=['Account Name_x', 'Account Name_y', 'Account_Name'], errors='ignore')
    print("Account Name columns consolidated.")
else:
    print("Account Name consolidation skipped: 'Account Name_x' not found (Account Name already clean).")

print(f"Shape after Account Name consolidation: {final_merged_data.shape}")
print(f"Columns after Account Name consolidation:\n{final_merged_data.columns.tolist()}")
print("\n--- Step 1 Complete ---")

# --- Running Step 2a: Handle Numerical Features (Fill with 0) ---
print("\n--- Running Step 2a: Handle Numerical Features (Fill with 0) ---")

numerical_cols_to_fill_0 = [
    'Mean_Signal_Score', 'Max_Signal_Score', 'Num_Unique_Topics',
    'Total_Opportunities', 'Total_Won_Opportunities', 'Total_Lost_Opportunities',
    'Engaged People', 'Engagement Minutes (3 mo.)',
    'Adobe Technologies Used', 'CVENT Technologies Used',
    'Total IAO Value', 'Ability to Pay',
    'Ever_Reached_Detected_Overall', 'Ever_Reached_Engaged_Overall',
    'Ever_Reached_Prioritized_Overall', 'Ever_Reached_Qualified_Overall',
    'Ever_Reached_Sales_Stages_Overall'
]
for col in numerical_cols_to_fill_0:
    if col in final_merged_data.columns:
        final_merged_data[col] = pd.to_numeric(final_merged_data[col], errors='coerce') # Ensure it's numeric, coerce errors to NaN
        final_merged_data[col] = final_merged_data[col].fillna(0)
    else:
        print(f"Warning: Numerical column '{col}' not found for filling with 0.")

print("Numerical feature cleaning complete. Checking NaN counts for these columns:")
print(final_merged_data[numerical_cols_to_fill_0].isnull().sum())
print("\n--- Step 2a Complete ---")

def is_effectively_empty(val):
    """
    Checks if a value is effectively empty:
    - NaN / None / pd.NaT
    - Empty list, tuple, array, or series
    - List/array/series of all NaNs
    Returns True if effectively empty, False otherwise.
    """
    import numpy as np
    import pandas as pd

    # 1. Handle non-iterables (scalars): Check if NaN or None
    if isinstance(val, (str, bytes)):
        return False  # Strings are not empty for this use case

    # Catch scalar NaNs safely
    if not hasattr(val, '__iter__'):
        return pd.isna(val)

    # 2. Handle iterables (excluding strings): convert to list
    try:
        val_as_list = list(val)
    except Exception:
        return pd.isna(val)  # fallback if conversion fails

    # Empty iterable
    if len(val_as_list) == 0:
        return True

    # Check if all elements are NaN
    return all(pd.isna(item) for item in val_as_list)

# --- Clean All_Unique_Topics ---
if 'All_Unique_Topics' in final_merged_data.columns:
    print("\n--- Running Pass 1 for 'All_Unique_Topics' ---")
    final_merged_data['All_Unique_Topics'] = [
        [] if is_effectively_empty(item)
        else (list(item) if hasattr(item, '__iter__') and not isinstance(item, (str, bytes)) else [item])
        for item in final_merged_data['All_Unique_Topics']
    ]

    print("Pass 1 done. Checking types and head:")
    print(final_merged_data['All_Unique_Topics'].apply(type).value_counts())
    print(final_merged_data['All_Unique_Topics'].head())

    print("\n--- Running Pass 2 for 'All_Unique_Topics' ---")
    final_merged_data['All_Unique_Topics'] = final_merged_data['All_Unique_Topics'].apply(
        lambda x: [] if is_effectively_empty(x) else x
    )
    print("Pass 2 done. Final check:")
    print(final_merged_data['All_Unique_Topics'].apply(type).value_counts())
    print(final_merged_data['All_Unique_Topics'].head())

# --- Clean Topic_With_Max_Score ---
if 'Topic_With_Max_Score' in final_merged_data.columns:
    print("\n--- Running Pass 1 for 'Topic_With_Max_Score' ---")
    final_merged_data['Topic_With_Max_Score'] = [
        [] if is_effectively_empty(item)
        else (list(item) if hasattr(item, '__iter__') and not isinstance(item, (str, bytes)) else [item])
        for item in final_merged_data['Topic_With_Max_Score']
    ]

    print("Pass 1 done. Checking types and head:")
    print(final_merged_data['Topic_With_Max_Score'].apply(type).value_counts())
    print(final_merged_data['Topic_With_Max_Score'].head())

    print("\n--- Running Pass 2 for 'Topic_With_Max_Score' ---")
    final_merged_data['Topic_With_Max_Score'] = final_merged_data['Topic_With_Max_Score'].apply(
        lambda x: [] if is_effectively_empty(x) else x
    )
    print("Pass 2 done. Final check:")
    print(final_merged_data['Topic_With_Max_Score'].apply(type).value_counts())
    print(final_merged_data['Topic_With_Max_Score'].head())

# --- Running Step 2c: Handle Categorical Features (Fill with 'No_Data') ---
print("\n--- Running Step 2c: Handle Categorical Features (Fill with 'No_Data') ---")

categorical_cols_to_fill_unknown = [
    'Type', 'Industry', 'Billing Country', 'Mktg Priority', 'Vertical',
    'Highest_Stage_Reached_Overall'
]
for col in categorical_cols_to_fill_unknown:
    if col in final_merged_data.columns:
        final_merged_data[col] = final_merged_data[col].fillna('No_Data')
    else:
        print(f"Warning: Categorical column '{col}' not found for filling with 'No_Data'.")

print("Categorical feature cleaning complete. Checking NaN counts for these columns:")
print(final_merged_data[categorical_cols_to_fill_unknown].isnull().sum())
print("\n--- Step 2c Complete ---")

# --- Running Step 2d: Handle Date Features (Create _Has_Date Flags) ---
print("\n--- Running Step 2d: Handle Date Features (Create _Has_Date Flags) ---")

date_cols_in_final_df = [
    'Latest_Opportunity_Close_Date', 'Latest_Activity_Date',
    'Max_Stage_Target_Date', 'Max_Stage_Detected_Date', 'Max_Stage_Engaged_Date',
    'Max_Stage_Prioritized_Date', 'Max_Stage_Qualified_Date', 'Max_Stage_Sales_Stages_Date'
]
for col in date_cols_in_final_df:
    if col in final_merged_data.columns:
        # Ensure they are datetime type, coercing errors to NaT
        final_merged_data[col] = pd.to_datetime(final_merged_data[col], errors='coerce')
        # Create the _Has_Date boolean flag
        final_merged_data[f"{col}_Has_Date"] = final_merged_data[col].notna()
        print(f"Processed date column: '{col}' and created '{col}_Has_Date'.")
    else:
        print(f"Warning: Date column '{col}' not found for _Has_Date flag creation.")

print("Date feature handling complete. Checking a sample of date columns and flags:")
# Display first few rows of a couple of date columns and their new flags
sample_date_cols = [col for col in date_cols_in_final_df if col in final_merged_data.columns][:2]
if sample_date_cols:
    sample_display_cols = []
    for col in sample_date_cols:
        sample_display_cols.append(col)
        sample_display_cols.append(f"{col}_Has_Date")
    pd.set_option('display.max_columns', None)
    print(final_merged_data[sample_display_cols].head())
    pd.reset_option('display.max_columns')
else:
    print("No date columns found to display.")

print("\n--- Step 2d Complete ---")

# --- Final Summary ---
print("\n--- Polishing Complete! ---")
print(f"New final_merged_data shape: {final_merged_data.shape}")
print(f"Columns after polishing:\n{final_merged_data.columns.tolist()}")

print("\nFinal NaN counts after polishing (should mostly be 0 for filled types, NaT for date originals):")
print(final_merged_data.isnull().sum())

print("\nFirst 5 rows of polished final_merged_data:")
pd.set_option('display.max_columns', None)
print(final_merged_data.head())
pd.reset_option('display.max_columns')

print("\nYour 'final_merged_data' DataFrame is now polished and ready for model building!")

final_merged_data

print(final_merged_data.isnull().sum())

Customer = final_merged_data['Type'].str.contains('Customer', case=False, na=False).sum()
Prospect = final_merged_data['Type'].str.contains('Prospect', case=False, na=False).sum()

print(f"Customer in Type column: {Customer}")
print(f"Prospect in Type column: {Prospect}")

print("--- Checking overlap of 'Customer' and 'Non-ICP' in firmo_modified4 ---")

# Filter for rows where 'Type' is 'Customer' AND 'Mktg Priority' is 'Non-ICP'
if 'firmo_modified4' in locals() and isinstance(firmo_modified4, pd.DataFrame):
    customer_non_icp_overlap = firmo_modified4[
        (firmo_modified4['Type'] == 'Customer') &
        (firmo_modified4['Mktg Priority'] == 'Non-ICP')
    ]

    # Count how many such rows exist
    overlap_count = customer_non_icp_overlap.shape[0]

    print(f"\nNumber of rows with 'Type' as 'Customer' AND 'Mktg Priority' as 'Non-ICP' in firmo_modified4: {overlap_count}")

    # Compare with the observed drop (101 to 95 = 6)
    observed_drop = 6 # Based on your previous observation
    if overlap_count == observed_drop:
        print(f"This count ({overlap_count}) perfectly matches the observed drop of {observed_drop} 'Customer' accounts.")
        print("This confirms your hypothesis: the removal of 'Non-ICP' rows caused those specific Customer accounts to be removed.")
    elif overlap_count > 0:
        print(f"There were {overlap_count} 'Customer' accounts also marked as 'Non-ICP'. This contributed to the drop.")
        print(f"Observed drop was {observed_drop} accounts. The difference might be due to other filtering or data nuances.")
    else:
        print("No 'Customer' accounts were found that were also 'Non-ICP'. The drop is due to another reason.")

    # Display these rows (optional)
    # if overlap_count > 0:
    #     print("\nSample of overlapping rows:")
    #     print(customer_non_icp_overlap.head())

else:
    print("Warning: 'firmo_modified4' DataFrame not found. Please load it first to perform this check.")

##export to csv
output_file_path = 'polished_final_data.csv'

print(f"--- Exporting final_merged_data to '{output_file_path}' ---")

try:
    # Export the DataFrame to CSV
    # index=False prevents pandas from writing the DataFrame's index as a column in the CSV
    final_merged_data.to_csv(output_file_path, index=False)
    print(f"Successfully exported final_merged_data to '{output_file_path}'")
except Exception as e:
    print(f"Error exporting DataFrame: {e}")

print("--- Export Complete ---")

excel_output_file_path = 'polished_final_data.xlsx'

print(f"--- Exporting final_merged_data to '{excel_output_file_path}' (Excel) ---")

try:
    # Export the DataFrame to Excel
    # index=False prevents pandas from writing the DataFrame's index as a column in the Excel file
    final_merged_data.to_excel(excel_output_file_path, index=False)
    print(f"Successfully exported final_merged_data to '{excel_output_file_path}'")
except Exception as e:
    print(f"Error exporting DataFrame to Excel: {e}")

print("--- Excel Export Complete ---")

## further cleaning creating numerical 'Days Ago' features from date columns


print("--- Starting Conversion of Date Columns to Numerical 'Days Ago' Features ---")

# Define the reference date for "days ago" calculation (e.g., today's date)
# Using a fixed date like pd.Timestamp.now() is fine for current analysis,
# but for a production model predicting future values, you might use the max date in your historical dataset
# to avoid 'future leakage' or a fixed 'model deployment date'. For now, let's use pd.Timestamp.now().
reference_date = pd.Timestamp.now()

# List of original date columns to convert to numerical 'Days Ago'
date_cols_to_convert_to_days_ago = [
    'Latest_Opportunity_Close_Date', 'Latest_Activity_Date',
    'Max_Stage_Target_Date', 'Max_Stage_Detected_Date', 'Max_Stage_Engaged_Date',
    'Max_Stage_Prioritized_Date', 'Max_Stage_Qualified_Date', 'Max_Stage_Sales_Stages_Date'
]

# A large number to indicate 'date not applicable' or 'very old/no activity'
# This should be larger than any possible real 'days ago' value.
NOT_APPLICABLE_DAYS = 9999

for col in date_cols_to_convert_to_days_ago:
    if col in final_merged_data.columns:
        # Calculate difference in days (will result in timedelta objects)
        time_difference = reference_date - final_merged_data[col]

        # Convert timedelta to number of days (float)
        # .dt.days extracts the integer number of days
        final_merged_data[f"{col}_Days_Ago"] = time_difference.dt.days

        # Fill NaNs in the new numerical 'Days Ago' column with NOT_APPLICABLE_DAYS
        # NaNs will appear here if the original date was NaT
        final_merged_data[f"{col}_Days_Ago"] = final_merged_data[f"{col}_Days_Ago"].fillna(NOT_APPLICABLE_DAYS)

        print(f"Created numerical feature: '{col}_Days_Ago'.")
    else:
        print(f"Warning: Original date column '{col}' not found. Skipping 'Days Ago' conversion.")

# --- Drop the original NaT-containing date columns ---
print("\n--- Dropping original date columns (now replaced by 'Days Ago' features and '_Has_Date' flags) ---")
columns_to_drop = [col for col in date_cols_to_convert_to_days_ago if col in final_merged_data.columns]
final_merged_data = final_merged_data.drop(columns=columns_to_drop)

print("Original date columns dropped.")
print(f"New final_merged_data shape after date transformation: {final_merged_data.shape}")

# --- You would then continue with your final print statements ---
# Final check of NaN counts and head of DataFrame will confirm the new numerical columns.

pd.set_option('display.max_columns', None)
final_merged_data

final_merged_data[final_merged_data.isna().any(axis=1)]
final_merged_data.isna().mean() * 100

final_merged_data.columns

cols = list(final_merged_data.columns)

# Remove 'Account Name' from the list
cols.remove('Account Name')

# Find the index of 'Account ID'
idx = cols.index('Account ID')

# Insert 'Account Name' right after 'Account ID'
cols.insert(idx + 1, 'Account Name')

# Reorder the DataFrame
final_merged_data = final_merged_data[cols]

# How many converted vs. not-converted in df1?
counts1 = final_merged_data['Account_Converted'].value_counts()
print(counts1)

##Building the Scoring Model

df1 = final_merged_data.copy()
df1.replace(9999, np.nan, inplace=True)
df1.replace("No_Data", np.nan, inplace=True)

import numpy as np

# 1) Assume df1 already exists and has your raw data
# 2) Define your target
y = df1['Account_Converted']

# 3) Start X by dropping pure identifiers and the target
X = df1.drop(columns=[
    'Account_Converted',
    'Account ID',
    'Account Name',
    'Type',                        # already-a-customer flag
    'Highest_Stage_Reached_Overall'  # direct leak
])

# 4) Identify postconversion columns by name patterns
leak_patterns = [
    'Opportunity', 'Won', 'Lost',     # any opportunity counts
    'Stage',                          # any stage flags
    'Ever_Reached',                   # boolean flags of ever reached
    '_Has_Date', '_Days_Ago'          # datebased recency/leak
]

# 5) Collect all columns matching those patterns
leak_cols = [
    c for c in X.columns
    if any(pat in c for pat in leak_patterns)
]

# 6) Drop them to get your true preconversion features
X_pre = X.drop(columns=leak_cols)

# 7) Show the result
print(f"Dropped {len(leak_cols)} post-conversion columns.")
print(f"Remaining pre-conversion features ({len(X_pre.columns)}):")
print(list(X_pre.columns))

# Starting from X_pre and y from before:

# 2a) Drop listvalued columns
drop_lists = ['All_Unique_Topics', 'Topic_With_Max_Score']
# 2b) Drop the remaining postconversion leak
drop_leak   = ['Total_Opportunities']

X_clean = X_pre.drop(columns=drop_lists + drop_leak)

print(f"Usable preconversion features ({len(X_clean.columns)}):")
print(list(X_clean.columns))

pd.set_option('display.max_columns', None)
df1

# How many converted vs. not-converted in df1?
counts = df1['Account_Converted'].value_counts()
print(counts)

##customer dates export

custodate_raw = pd.read_excel("custodate.xlsx", header=9)

## check how many rows have Customer in Type column in custodate_raw
customer_count_raw = custodate_raw['Type'].str.contains('Customer', case=False, na=False).sum()
print(f"Number of rows with 'Customer' in Type column in custodate_raw: {customer_count_raw}")

# 1) Keep only rows whose Account ID exists in df1
custodate_filtered = custodate_raw[
    custodate_raw['Account ID'].isin(df1['Account ID'])
].copy()

# 2) Drop any rows missing Account ID just in case
custodate_filtered = custodate_filtered[custodate_filtered['Account ID'].notna()]

# 3) Deduplicate on Account ID (keeping the first occurrence)
custodate_filtered = custodate_filtered.drop_duplicates(
    subset='Account ID',
    keep='first'
)

# 4) Check
print("Filtered shape:", custodate_filtered.shape)
print("Unique Account IDs:", custodate_filtered['Account ID'].nunique())

df1.shape
custodate_filtered.columns

# 1) Select only the columns we care about
cust_clean = custodate_raw[[
    'Account ID',
    'Customer Date'
]].copy()

# 2) Rename & parse dates
cust_clean = cust_clean.rename(columns={'Customer Date':'Conversion_Date'})
cust_clean['Conversion_Date'] = pd.to_datetime(
    cust_clean['Conversion_Date'],
    errors='coerce',
    infer_datetime_format=True
)

# 3) Quick check
print("cust_clean columns:", list(cust_clean.columns))
print(cust_clean.head())

cust_simple = cust_clean[['Account ID','Conversion_Date']].copy()
cust_simple = cust_simple.drop_duplicates(subset='Account ID', keep='first')
df1 = df1.merge(
    cust_simple,
    on='Account ID',
    how='left'
)

df1.columns

# 1) Combine the two into one, preferring the merged-in dates
df1['Conversion_Date'] = df1['Conversion_Date_y'].combine_first(df1['Conversion_Date_x'])

# 2) Drop the old columns
df1 = df1.drop(columns=['Conversion_Date_x','Conversion_Date_y'])

# 3) Verify
print("df1 shape:", df1.shape)
print("Conversion_Date non-null:", df1['Conversion_Date'].notna().sum(), "of", len(df1))

# Count before
before = len(df1)

# Drop the 14 converted accounts missing dates
df1 = df1[~((df1['Account_Converted'] == 1) & df1['Conversion_Date'].isna())].copy()

# Verify
after = len(df1)
print(f"Dropped {before - after} accounts; new df1 shape: {df1.shape}")
print("Remaining converted accounts:", df1['Account_Converted'].sum())
print("Remaining total accounts:   ", len(df1))

## check for nan in df1
print("Checking for NaNs in df1:")
nan_counts = df1.isna().sum()
print(nan_counts[nan_counts > 0])

import pandas as pd

# 1) Define snapshot date for non-converters
snapshot = pd.Timestamp('2025-07-28')

# 2) Build the cutoff column in df1
df1['cutoff'] = df1['Conversion_Date'].fillna(snapshot)

# 3) Verify by printing a few rows
print(df1[['Account ID','Account_Converted','Conversion_Date','cutoff']].head(30))

df1.head(30)

##exporting other dates

otherdates_raw = pd.read_excel("otherdates.xlsx", header=9)

otherdates_raw

# 1) Start from your loaded DataFrame
od = otherdates_raw.copy()

# 2) Drop the helper Unnamed columns
od = od.drop(columns=['Unnamed: 0','Unnamed: 2'])

# 3) Remove the trailing Total/Sum/Count rows
#    We keep only rows whose Account ID appears in df1
od = od[od['Account ID'].isin(df1['Account ID'])].copy()

# List the raw date columns
date_cols = [
    'Created Date',
    'First Engagement Date',
    'Person Account: First Call Date',
    'Last Engagement Date',
    'MQA Date',
    'Pipeline Date'
]

# 4) Convert each to datetime (coercing errors  NaT)
for col in date_cols:
    od[col] = pd.to_datetime(
        od[col],
        errors='coerce',
        infer_datetime_format=True
    )

# 5) Rename the # Days Since Last Activity column to a Pythonfriendly name
od = od.rename(columns={'Days Since Last Activity':'days_since_last_activity'})

od

# 1) Drop everything except Account ID + our new columns
keep_cols = ['Account ID'] + date_cols + ['days_since_last_activity']
od_clean = od[keep_cols].copy()

# 2) Filter to only the accounts present in df1
od_clean = od_clean[od_clean['Account ID'].isin(df1['Account ID'])].copy()

# 3) Drop any duplicates (just in case)
od_clean = od_clean.drop_duplicates(subset='Account ID', keep='first')

# 4) Check we didnt lose anything
print("od_clean shape:", od_clean.shape)
print("Unique IDs in od_clean:", od_clean['Account ID'].nunique(), "of", len(df1))

# 1a) Merge od_clean into df1 (left-join keeps all df1 rows)
df1 = df1.merge(
    od_clean,
    on='Account ID',
    how='left'
)

# 1b) Verify shapes
print("After merge, df1 shape:", df1.shape)
# Should still be (7626, ) and now include your 6 date cols + days_since_last_activity

# 2a) Define your date columns
date_cols = [
    'Created Date',
    'First Engagement Date',
    'Person Account: First Call Date',
    'Last Engagement Date',
    'MQA Date',
    'Pipeline Date'
]

# 2b) Build the recency features
for col in date_cols:
    newcol = 'days_since_' + col.lower().replace(' ','_').replace(':','')
    df1[newcol] = (df1['cutoff'] - df1[col]).dt.days

# 2c) Preview the new features
recency_cols = [ 'days_since_' + c.lower().replace(' ','_').replace(':','') for c in date_cols ] + ['days_since_last_activity']
print(df1[recency_cols].head())

# 3a) List your static features
static_feats = [
    'Annual Revenue',
    'Industry',
    'Billing Country',
    'Mktg Priority',
    'Adobe Technologies Used',
    'CVENT Technologies Used',
    'Ability to Pay',
    'Vertical'
]

# 3b) List your recency features
recency_feats = [
    'days_since_created_date',
    'days_since_first_engagement_date',
    'days_since_person_account_first_call_date',
    'days_since_last_engagement_date',
    'days_since_mqa_date',
    'days_since_pipeline_date',
    'days_since_last_activity'
]

# 3c) Combine into one feature list
feature_cols = static_feats + recency_feats

# 3d) Define X and y
X = df1[feature_cols]
y = df1['Account_Converted']

# 3e) Inspect the result
print("X.shape:", X.shape)
print("y distribution:\n", y.value_counts(normalize=True))
print("\nX.head():")
print(X.head())

# Compute % missing for each feature in X
missing_pct = X.isna().mean() * 100
print(missing_pct.sort_values(ascending=False))

# 1) Define the recencies to keep
good_recency = [
    'days_since_created_date',
    'days_since_first_engagement_date',
    'days_since_last_engagement_date',
    'days_since_last_activity'
]

# 2) Static features (no missing)
static_feats = [
    'Annual Revenue',
    'Industry',
    'Billing Country',
    'Mktg Priority',
    'Adobe Technologies Used',
    'CVENT Technologies Used',
    'Ability to Pay',
    'Vertical'
]

# 3) Combine
feature_cols = static_feats + good_recency
X_final = df1[feature_cols]
y        = df1['Account_Converted']

# 4) Quick sanity checks
print("Final X shape:", X_final.shape)
print("Final features:\n", feature_cols)
print("Missing % in X_final:\n", (X_final.isna().mean()*100).round(2))
print("Target balance:\n", y.value_counts(normalize=True))

# List the raw date columns you merged in
date_cols = [
    'Created Date',
    'First Engagement Date',
    'Person Account: First Call Date',
    'Last Engagement Date',
    'MQA Date',
    'Pipeline Date'
]

# For each account, if the raw date is *after* cutoff, drop it (set to NaT)
for col in date_cols:
    df1[col] = df1[col].where(
        df1[col] <= df1['cutoff'],
        pd.NaT
    )

pd.set_option('display.max_columns', None)
df1

df1.rename(columns={
    'Max_Stage_Idx_PreConv_x': 'Max_Stage_Idx_PreConv',
    'Num_Stage_Changes_x':     'Num_Stage_Changes',
    'Last_Stage_Date_PreConv_x':'Last_Stage_Date_PreConv'
}, inplace=True)

df1.drop(columns=[
    'Max_Stage_Idx_PreConv_y',
    'Num_Stage_Changes_y',
    'Last_Stage_Date_PreConv_y'
], inplace=True)

leaky = [col for col in df1.columns if 'Days_Ago' in col or col.endswith('_Has_Date')
         or 'Activity_Date' in col or 'Opportunity_Close' in col]
df1.drop(columns=leaky, inplace=True)

df1.drop(columns=['Billing_Country_Binned'], inplace=True)

df1[['Billing Country']].head()

df1.columns

df1 = df1.drop(columns=[
    'Max_Stage_Idx_PreConv',
    'Last_Stage_Date_PreConv',
    'Num_Stage_Changes',
    'days_since_last_stage'
])

df1.columns

df1 = df1.drop(columns=[
    'stage_idx_cleaned_x',
       'last_stage_change_date_cleaned_x', 'days_since_last_stage_cleaned',
       'stage_idx_cleaned_y', 'last_stage_change_date_cleaned_y'
])

## exporting stage date

stagedate_raw = pd.read_excel("stagedate.xlsx", header=10)
stagedate_raw

## check nans in Stage in stagedate_raw
stage_nan_count = stagedate_raw['Stage'].isna().sum()
print(f"Number of NaNs in 'Stage' column in stagedate_raw: {stage_nan_count}")

import pandas as pd

#  1) Build per-account cutoff 
# ensure Conversion_Date is datetime
df1['Conversion_Date'] = pd.to_datetime(df1['Conversion_Date'], errors='coerce')

# parse Last Stage Change Date in your raw history
stagedate_raw['Last Stage Change Date'] = pd.to_datetime(
    stagedate_raw['Last Stage Change Date'], errors='coerce'
)
# snapshot = the very last historical date weve seen
snapshot = stagedate_raw['Last Stage Change Date'].max()
print("Snapshot (last available pre-cutoff date):", snapshot)

# cutoff = one day before conversion for actual customers, else snapshot for prospects
df1['cutoff'] = df1.apply(
    lambda r: (r['Conversion_Date'] - pd.Timedelta(days=1))
              if pd.notna(r['Conversion_Date'])
              else snapshot,
    axis=1
)


#  2) Clean & filter your stage history 
st = (
    stagedate_raw
    .drop(columns=['Unnamed: 0','Unnamed: 2'])
    .query("`Account ID` != 'Total'")
    .loc[:, ['Account ID','Stage','Last Stage Change Date']]
    .dropna(subset=['Stage','Last Stage Change Date'])
    .merge(df1[['Account ID','cutoff']], on='Account ID', how='inner')
)

# only keep pre-cutoff events
sd_pre = st[ st['Last Stage Change Date'] <= st['cutoff'] ]


#  3) Exclude the two final stages 
FINAL = {'Closed Won','Closed Lost'}
sd_pre_active = sd_pre[ ~sd_pre['Stage'].isin(FINAL) ]


#  4) Map only your **active** stages  integers 
stage_order_active = [
  "Research", "Target Demand", "Detected Demand", "Engaged Demand",
  "Prioritized Demand", "Qualified Demand", "Discovery", "Proposal",
  "Selected", "Contracts"
]
idx_map = {stage: i for i,stage in enumerate(stage_order_active)}

# sanity check for typos
unknown = set(sd_pre_active['Stage']) - set(idx_map)
if unknown:
    print(" Unknown stages found:", unknown)

sd_pre_active['stage_idx_temp'] = sd_pre_active['Stage'].map(idx_map)

# per-account aggregates
max_stage = (
    sd_pre_active
    .groupby('Account ID')['stage_idx_temp']
    .max()
    .rename('stage_idx_cleaned')
)
last_date = (
    sd_pre_active
    .groupby('Account ID')['Last Stage Change Date']
    .max()
    .rename('last_stage_change_date_cleaned')
)


#  5) Drop any old/leaky columns & merge back 
to_drop = [
    'stage_idx','days_since_last_stage',
    'stage_idx_cleaned','last_stage_change_date_cleaned'
]
for c in to_drop:
    if c in df1: df1.drop(columns=c, inplace=True)

df1 = (
    df1
    .merge(max_stage, on='Account ID', how='left')
    .merge(last_date, on='Account ID', how='left')
)

# fill missing:
df1['stage_idx_cleaned'] = df1['stage_idx_cleaned'].fillna(0).astype(int)

big = int((df1['cutoff'] - snapshot).dt.days.max()) + 1
df1['days_since_last_stage_cleaned'] = (
    df1['cutoff'] - df1['last_stage_change_date_cleaned']
).dt.days.fillna(big).astype(int)


print(" Done. Leak-free stage features are now in df1:")
print("    stage_idx_cleaned   [0%d]" % (len(stage_order_active)-1))
print("    days_since_last_stage_cleaned  (filled with %d for never moved)" % big)

df1[['stage_idx_cleaned','days_since_last_stage_cleaned']].head()

# How many unique accounts ever show up in your active-stage table?
print(sd_pre_active['Account ID'].nunique(), "accounts with active-stage events")
print(len(df1[df1.Type=='Prospect']),        "total prospects")

print(df1['stage_idx_cleaned'].value_counts().sort_index())

## check na in df1print("Checking for NaNs in df1:")
nan_counts = df.isna().sum()
print(nan_counts[nan_counts > 0])

from sklearn.base import BaseEstimator, TransformerMixin

class StageBooster(BaseEstimator, TransformerMixin):
    def __init__(self, multiplier=1.0):
        self.multiplier = multiplier
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        X = X.copy()
        # create boosted column and drop the raw one
        X['stage_idx_boosted'] = X['stage_idx_cleaned'] * self.multiplier
        return X[[
            # reorder so that downstream only sees boosted
            'stage_idx_boosted',
            'days_since_first_engagement_date',
            'Stage_x_Engagement_Days',
            'Stage_x_Created_Days',
            'Billing Country',
            'Industry'
        ]]

import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from xgboost import XGBClassifier
from sklearn.metrics import (
    roc_auc_score, f1_score, average_precision_score,
    classification_report, confusion_matrix, ConfusionMatrixDisplay
)
import matplotlib.pyplot as plt
import re

# 
# 0) ASSUME df1 is already loaded & has all the cleaned stage features:
#    - df1['stage_idx_cleaned']
#    - interaction features like 'Tech_Vertical_Stage_Interaction', etc.
#    - df1['Account_Converted'] binary target
# 

# 1) Define X and y
features = [
            ''
            'stage_idx_cleaned',
            'days_since_created_date',
            'days_since_first_engagement_date',
            'Stage_x_Engagement_Days',
            'Stage_x_Created_Days',
            'Billing Country',
            'Vertical',
            'Industry'
]
X = df1[features].copy()
y = df1['Account_Converted']

# 2) Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# compute scale_pos_weight to handle class imbalance
scale_pos_weight_value = (y_train==0).sum() / (y_train==1).sum()

# 
# 3) Create a tiny transformer to boost the stage index by a tunable multiplier
# 
class StageBooster(BaseEstimator, TransformerMixin):
    def __init__(self, multiplier=1.0):
        self.multiplier = multiplier
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        # X will be a DataFrame with at least 'stage_idx_cleaned'
        X = X.copy()
        X['stage_idx_boosted'] = X['stage_idx_cleaned'] * self.multiplier
        # drop the raw one so downstream only sees the boosted version
        return X.drop(columns=['stage_idx_cleaned'])

# 
# 4) Build the preprocessing steps
# 
numeric_feats = [
    'stage_idx_boosted',
    'days_since_created_date',
    'days_since_first_engagement_date',
    'Stage_x_Engagement_Days',
    'Stage_x_Created_Days'
]
cat_feats = ['Billing Country', 'Vertical', 'Industry']

numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler',  StandardScaler()),
])
categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False)),
])

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_feats),
    ('cat', categorical_transformer, cat_feats),
])

# 
# 5) Build the full model pipeline
# 
xgb_base = XGBClassifier(
    objective='binary:logistic',
    eval_metric='aucpr',
    use_label_encoder=False,
    scale_pos_weight=scale_pos_weight_value,
    random_state=42,
    monotone_constraints="(1, 0, 0, 0, 0, 0, 0, 0, 0)"
    # 1 for stage_idx_boosted  force non-decreasing in that feature
)

pipeline = Pipeline([
    ('boost',  StageBooster()),   # default multiplier=1.0
    ('preproc',preprocessor),
    ('clf',    xgb_base),
])

# 
# 6) Hyperparameter search, including the stage multiplier
# 
param_dist = {
    'boost__multiplier':    [1, 5, 10, 20, 50, 100, 200, 500],
    'clf__n_estimators':      [100,150,200,300],
    'clf__learning_rate':     [0.05,0.1,0.15,0.2],
    'clf__max_depth':         [3,4,5],
    'clf__min_child_weight':  [1,2,5,10],
    'clf__gamma':             [0.1,0.5,1.0],
    'clf__subsample':         [0.7,0.8,0.9],
    'clf__colsample_bytree':  [0.7,0.8,0.9],
    'clf__reg_lambda':        [1,5,10,50],
    'clf__reg_alpha':         [0,0.1,0.5,1,5],
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
search = RandomizedSearchCV(
    pipeline, param_dist,
    n_iter=100, cv=cv,
    scoring='f1',
    verbose=2, n_jobs=-1, random_state=42
)
search.fit(X_train, y_train)

best_pipe = search.best_estimator_
print(" Best params:", search.best_params_)
print(" Best CV F1:", search.best_score_)

# 
# 7) Final evaluation on the holdout set
# 
y_proba = best_pipe.predict_proba(X_test)[:,1]
y_pred  = (y_proba > 0.5).astype(int)

print(classification_report(y_test, y_pred, digits=4))
print("ROC AUC:", roc_auc_score(y_test, y_proba))
print("PR  AUC:", average_precision_score(y_test, y_proba))

ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred,
    display_labels=['Not Converted','Converted'],
    cmap=plt.cm.Blues
)
plt.title("Confusion Matrix (threshold=0.5)")
plt.show()

# 
# 8) Inspect top XGB gain importances
# 
clf_final = best_pipe.named_steps['clf']
ohe_names = best_pipe.named_steps['preproc'] \
    .named_transformers_['cat'] \
    .named_steps['ohe'] \
    .get_feature_names_out(cat_feats)

all_features = numeric_feats + list(ohe_names)
gain = clf_final.get_booster().get_score(importance_type='gain')
imp = { all_features[int(re.sub(r"^f","",k))]: v for k,v in gain.items() }
imp = pd.Series(imp).sort_values(ascending=False)
print("\nTop features by gain:\n", imp.head(10))

import pandas as pd
import numpy as np

# 
# 1) Subset to prospects only
# 
prospects = df1[df1['Account_Converted'] == 0].copy()

# 
# 2) Prepare X exactly as before
# 
features = [
    'days_since_created_date',
    'days_since_first_engagement_date',
    'Stage_x_Engagement_Days',
    'Stage_x_Created_Days',
    'Vertical',
    'Billing Country',
    'Industry',
    'stage_idx_cleaned'  # use the cleaned stage index
]
X_prospects = prospects[features].copy()

# 
# 3) Predict conversion probabilities (0100%)
# 
probs = best_pipe.predict_proba(X_prospects)[:, 1] * 100
prospects['Conversion Probability'] = probs

# 
# 4) Sort descending and assign Ranking
# 
prospects = prospects.sort_values('Conversion Probability', ascending=False)
prospects['Ranking'] = np.arange(1, len(prospects) + 1)



# 
# 5) Compute Percentile 1100
#    (1 = top, 100 = bottom)
# 
N = len(prospects)
r = prospects['Ranking']
# linear interpolation so rank=1  pct=1, rank=N  pct=100
prospects['Percentile'] = (1 + (r - 1) * (99 / (N - 1))).round().astype(int)



print(prospects[['Account Name', 'Conversion Probability','Ranking','Percentile', 'stage_idx_cleaned']].head(10))

mask = (df1['Has_Stage_Progression'] == 1) & (df1['stage_idx_cleaned'] == 0)
print(f"Count of those cases: {mask.sum()}")
display(df1.loc[mask, [
    'Account Name',
    'stage_idx_cleaned',
    'Has_Stage_Progression',
    'Stage_x_Engagement_Days',
    'Stage_x_Created_Days'
]].head(30))

mask = (df1['Has_Stage_Progression'] == 1) & (df1['stage_idx_cleaned'] == 0)
print(f"Count: {mask.sum()}")
display(df1.loc[mask, ['Account Name', 'stage_idx_cleaned', 'Has_Stage_Progression',
                      'Stage_x_Engagement_Days', 'Stage_x_Created_Days']].head(20))

## Check for ranking of Account ICANN
account_icann = prospects[prospects['Account Name'] == 'Reed Exhibitions']
if not account_icann.empty:
    print("\nICANN Account Ranking:")
    print(account_icann[['Account Name', 'Conversion Probability', 'Ranking', 'Percentile']])
else:
    print("\nICANN Account not found in prospects data.")

num_above_50 = (prospects['Conversion Probability'] > 50).sum()
print(f"{num_above_50} accounts have a conversion probability above 50%.")
# If `prospects` is the DataFrame youve added the
# Conversion Probability / Ranking / Percentile columns to:
total_prospects = len(prospects)
print(f"We have {total_prospects} prospects in total.")

total_original = df1.shape[0]
print(f"The original dataset has {total_original} prospects.")

## Numerical list  with conversion probability of all prospects and prospects with stage progression

## export df1 tp csv
output_file_path = 'final_merged_data.csv'
print(f"--- Exporting df1 to '{output_file_path}' ---")
try:
    # Export the DataFrame to CSV
    df1.to_csv(output_file_path, index=False)
    print(f"Successfully exported df1 to '{output_file_path}'")
except Exception as e:
    print(f"Error exporting DataFrame: {e}")

df1.columns

